"""
ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- MNIST ë°ì´í„°ì…‹ ë¡œë“œ
- ëª¨ë¸ í•™ìŠµ
- MLflowë¡œ ì‹¤í—˜ ì¶”ì 
"""

import argparse
import os
from datetime import datetime

import mlflow
import mlflow.pytorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

from model import count_parameters, get_model


def get_data_loaders(batch_size: int = 64, data_dir: str = "../data"):
    """MNIST ë°ì´í„° ë¡œë” ìƒì„±"""
    transform = transforms.Compose(
        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
    )

    train_dataset = datasets.MNIST(
        root=data_dir, train=True, download=True, transform=transform
    )

    test_dataset = datasets.MNIST(
        root=data_dir, train=False, download=True, transform=transform
    )

    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2
    )

    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2
    )

    return train_loader, test_loader


def train_epoch(model, train_loader, criterion, optimizer, device):
    """í•œ ì—í­ í•™ìŠµ"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for data, target in train_loader:
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

    avg_loss = running_loss / len(train_loader)
    accuracy = 100.0 * correct / total

    return avg_loss, accuracy


def evaluate(model, test_loader, criterion, device):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

    avg_loss = test_loss / len(test_loader)
    accuracy = 100.0 * correct / total

    return avg_loss, accuracy


def train(args):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  Using device: {device}")

    # MLflow ì„¤ì •
    mlflow.set_tracking_uri(args.mlflow_uri)
    mlflow.set_experiment(args.experiment_name)

    with mlflow.start_run(run_name=f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_params(
            {
                "batch_size": args.batch_size,
                "epochs": args.epochs,
                "learning_rate": args.lr,
                "optimizer": "Adam",
                "device": str(device),
            }
        )

        # ë°ì´í„° ë¡œë”
        train_loader, test_loader = get_data_loaders(
            batch_size=args.batch_size, data_dir=args.data_dir
        )
        print(f"ğŸ“Š Train samples: {len(train_loader.dataset)}")
        print(f"ğŸ“Š Test samples: {len(test_loader.dataset)}")

        # ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €
        model = get_model(num_classes=10).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=args.lr)

        # ëª¨ë¸ ì •ë³´ ë¡œê¹…
        mlflow.log_param("model_parameters", count_parameters(model))

        # í•™ìŠµ ë£¨í”„
        best_accuracy = 0.0

        for epoch in range(1, args.epochs + 1):
            train_loss, train_acc = train_epoch(
                model, train_loader, criterion, optimizer, device
            )

            test_loss, test_acc = evaluate(model, test_loader, criterion, device)

            # ë©”íŠ¸ë¦­ ë¡œê¹…
            mlflow.log_metrics(
                {
                    "train_loss": train_loss,
                    "train_accuracy": train_acc,
                    "test_loss": test_loss,
                    "test_accuracy": test_acc,
                },
                step=epoch,
            )

            print(f"Epoch {epoch}/{args.epochs}")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")

            # Best ëª¨ë¸ ì €ì¥
            if test_acc > best_accuracy:
                best_accuracy = test_acc
                model_path = os.path.join(args.model_dir, "best_model.pth")
                torch.save(
                    {
                        "epoch": epoch,
                        "model_state_dict": model.state_dict(),
                        "optimizer_state_dict": optimizer.state_dict(),
                        "accuracy": test_acc,
                    },
                    model_path,
                )
                print(f"  âœ… Best model saved! (Accuracy: {test_acc:.2f}%)")

        # ìµœì¢… ëª¨ë¸ MLflowì— ì €ì¥
        mlflow.pytorch.log_model(model, "model")
        mlflow.log_metric("best_accuracy", best_accuracy)

        print(f"\nğŸ‰ Training completed! Best accuracy: {best_accuracy:.2f}%")
        print(f"ğŸ“ Model saved to: {args.model_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MNIST ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ")

    parser.add_argument("--batch-size", type=int, default=64, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--epochs", type=int, default=5, help="ì—í­ ìˆ˜")
    parser.add_argument("--lr", type=float, default=0.001, help="í•™ìŠµë¥ ")

    parser.add_argument("--data-dir", type=str, default="../data", help="ë°ì´í„° ê²½ë¡œ")
    parser.add_argument(
        "--model-dir", type=str, default="../models", help="ëª¨ë¸ ì €ì¥ ê²½ë¡œ"
    )

    parser.add_argument(
        "--mlflow-uri", type=str, default="mlruns", help="MLflow ì¶”ì  URI"
    )
    parser.add_argument(
        "--experiment-name",
        type=str,
        default="mnist-classification",
        help="ì‹¤í—˜ ì´ë¦„",
    )

    args = parser.parse_args()

    os.makedirs(args.model_dir, exist_ok=True)

    train(args)
