"""
ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€
- ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
"""

import argparse
import json
import os

import torch
from sklearn.metrics import classification_report, confusion_matrix
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

from model import get_model


def load_model(model_path: str, device: torch.device):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    model = get_model(num_classes=10)

    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device, weights_only=True)
        model.load_state_dict(checkpoint["model_state_dict"])
        print(f"âœ… Model loaded from {model_path}")
        print(f"   Trained accuracy: {checkpoint.get('accuracy', 'N/A')}%")
    else:
        raise FileNotFoundError(f"Model not found at {model_path}")

    model.to(device)
    model.eval()
    return model


def get_test_loader(batch_size: int = 64, data_dir: str = "../data"):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”"""
    transform = transforms.Compose(
        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
    )

    test_dataset = datasets.MNIST(
        root=data_dir, train=False, download=True, transform=transform
    )

    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2
    )

    return test_loader


def evaluate_model(model, test_loader, device):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    all_predictions = []
    all_targets = []
    correct = 0
    total = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            _, predicted = output.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

            total += target.size(0)
            correct += predicted.eq(target).sum().item()

    accuracy = 100.0 * correct / total

    # Classification Report
    report = classification_report(
        all_targets,
        all_predictions,
        target_names=[str(i) for i in range(10)],
        output_dict=True,
    )

    # Confusion Matrix
    cm = confusion_matrix(all_targets, all_predictions)

    return {
        "accuracy": round(accuracy, 2),
        "total_samples": total,
        "correct_predictions": correct,
        "classification_report": report,
        "confusion_matrix": cm.tolist(),
    }


def main(args):
    """ë©”ì¸ í•¨ìˆ˜"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  Using device: {device}")

    # ëª¨ë¸ ë¡œë“œ
    model = load_model(args.model_path, device)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
    test_loader = get_test_loader(data_dir=args.data_dir)
    print(f"ğŸ“Š Test samples: {len(test_loader.dataset)}")

    # í‰ê°€ ì‹¤í–‰
    print("\nğŸ” Evaluating model...")
    results = evaluate_model(model, test_loader, device)

    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“ˆ Evaluation Results:")
    print(f"   Accuracy: {results['accuracy']}%")
    print(f"   Correct: {results['correct_predictions']}/{results['total_samples']}")

    # ê²°ê³¼ ì €ì¥
    output_path = os.path.join(args.output_dir, "evaluation_results.json")
    os.makedirs(args.output_dir, exist_ok=True)

    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\nğŸ’¾ Results saved to: {output_path}")

    # í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬
    if results["accuracy"] >= args.min_accuracy:
        print(f"âœ… Model PASSED quality gate (>= {args.min_accuracy}%)")
        return 0
    else:
        print(f"âŒ Model FAILED quality gate (< {args.min_accuracy}%)")
        return 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ëª¨ë¸ í‰ê°€")

    parser.add_argument(
        "--model-path",
        type=str,
        default="../models/best_model.pth",
        help="ëª¨ë¸ íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument("--data-dir", type=str, default="../data", help="ë°ì´í„° ê²½ë¡œ")
    parser.add_argument(
        "--output-dir", type=str, default="../models", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--min-accuracy",
        type=float,
        default=95.0,
        help="ìµœì†Œ ì •í™•ë„ ì„ê³„ê°’ (%)",
    )

    args = parser.parse_args()

    exit_code = main(args)
    exit(exit_code)
